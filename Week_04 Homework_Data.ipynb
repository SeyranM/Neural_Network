{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Տնայինի այս մասում փորձելու եք իրականացնել backpropagation (BP) ալգորիթմը:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BP ալգորիթմի նպատակն է այնպես փոփոխել մեր ցանցի պարամետրերը՝ W-երը և b-երը, որ loss-ը մաքսիմալ փոքր լինի: Այսինքն՝ եթե ծաղիկը A տեսակի է, ապա մենք ուզում ենք, որ ցանցին այդ ծաղիկի ցողունի բարձրությունը և պսակի տրամագիծը տալով, այն ինչքան հնարավոր է (1,0)-ին մոտ output տա, օրինակ՝ (0.98, 0.15)-ը շատ լավ արդյունք է, քանի որ թվերին նայելով կարող ենք ասել, որ ցանցը վստահ է, որ ծաղիկը A տեսակի է:\n",
    "\n",
    "<font color='green'>Նշում:</font> Ինչպե՞ս ենք որոշում ցանցը ինչ է գուշակում՝ A, թե B: Դրա համար մոտեցումը շատ պարզ է. համեմատում ենք output-ի երկու թվերը և կախված, թե որն է մեծ, ասում ենք ցանցը գուշակում է A, եթե առաջին թիվը մեծ է երկրորդից և B` հակառակ դեպքում:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Գիտենք, որ loss կամ cost ֆունկցիան ֆունկցիա է ցանցի պարամետրերից կախված: Իրականում այն ֆունկցիա է ոչ միայն պարամետրերից, այլև input(X)-ից և target(Y)-ից, բայց քանի որ սրանք տրված են, ապա այն կոնկրետ training example-ի համար ֆունկցիա է միայն պարամետրերից: Քանի որ պարամետրերի քանակը շատ մեծ է և կախվածությունն էլ բավականին բարդ տեսքի է, ապա մենք անալիտիկ տեսքով չենք կարող ստանալ W-երի և b-երի այն արժեքները, որոնց դեպքում loss-ը մինիմում է:\n",
    "<br> Այդ նատակով մենք օգտագործում ենք իտերատիվ ալգորիթմ՝ Gradient descent (GD), որի իմաստը հետևյալն է՝\n",
    "-  նախ հաշվում ենք Cost function (CF)-ի մասնակի ածանցյալները ըստ բոլոր պարամետրերի, այլ կերպ ասած՝ գրադիենտները,\n",
    "-  ապա բոլոր պարամետրերի արժեքները update ենք անում հետևյալ բանաձևի միջոցով՝ W_new = W_old - alpha * grad_W, նմանապես բոլոր b-երի համար (alpha-ն learning rate(lr)-ն է, որի միջոցով կառավարում ենք ցանցի սովորելու արագությունը, սովորաբար դրվում է փոքր թիվ, օրինակ՝ 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Սկզբի համար W-ների և b-երի գրադիենտները կհաշվենք միայն առաջին ծաղիկի համար:__\n",
    "<br><font color='red'> Քանի որ մեր օրինակը շատ նման է հոդվածի թվային օրինակին, հետևաբար հիմնական բանաձևերը և դրանց դուրսբերումը պետք է նայել հոդվածում:</font>\n",
    "<br>    https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='magenta'> ՀԱՏԿԱՊԵՍ ԱՅՍՏԵՂ Է ՊԵՏՔ ԳԱԼՈԻ ՁԵՐ ՆԿԱՐԱԾ ՑԱՆՑԸ:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><font color='Blue'> Նշում: Այսուհետ Cost function-ի մասնակի ածանցյալը ըստ որևէ W-ի, b-ի կամ output-ի ուղղակի կանվանենք տվյալ __W-ի, b-ի կամ output-ի գրադիենտ__, համապատասխանաբար Cost function-ի մասնակի ածանցյալը ըստ որևէ շերտի նեյրոնների կանվանենք __տվյալ շերտի նեյրոնների գրադիենտ__:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.array([[ 0.51,  0.41],\n",
    "               [ 0.34,  0.24],\n",
    "               [ 0.25, -0.14],\n",
    "               [-0.62, -0.42]])\n",
    "\n",
    "b1 = np.array([-0.65,  0.27,  0.58,  0.16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = np.array([[-0.41, -0.38,  0.22,  0.1 ],\n",
    "            [-0.16,  0.3 ,  0.02,  0.03]])\n",
    "\n",
    "b2 = np.array([0.49, 0.41])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[26.86,  6.86],\n",
    "               [29.48,  8.57],\n",
    "               [29.63,  8.01],\n",
    "               [33.74,  7.  ],\n",
    "               [18.54,  6.97],\n",
    "               [15.98,  5.69],\n",
    "               [19.01,  4.5 ]])\n",
    "\n",
    "\n",
    "Y = np.array([[\"A\"],\n",
    "              [\"A\"],\n",
    "              [\"A\"],\n",
    "              [\"A\"],\n",
    "              [\"B\"],\n",
    "              [\"B\"],\n",
    "              [\"B\"],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:,0] -= np.mean(X[:,0])\n",
    "X[:,1] -= np.mean(X[:,1])\n",
    "\n",
    "X[:,0] /= np.var(X[:,0])\n",
    "X[:,1] /= np.var(X[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array ([[1,0] if i[0]==\"A\" else [0,1] for i in Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__ (self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.output = None\n",
    "        \n",
    "    def forward(self, inpt):\n",
    "        self.output = np.dot(inpt, self.W.T) + self.b\n",
    "        return self.output\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        \n",
    "    def forward(self, inpt):\n",
    "        self.output = 1/(1 + np.exp(-inpt))\n",
    "        return self.output\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSECriterion():\n",
    "    def __init__ (self):\n",
    "        self.output = None\n",
    "        \n",
    "    def forward(self, inpt, target):\n",
    "        self.output = np.mean((inpt - target)**2)\n",
    "        return self.output\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer1_linear = Linear(W1, b1)\n",
    "Layer1_sigmoid = Sigmoid()\n",
    "Layer2_linear = Linear(W2, b2)\n",
    "Layer2_sigmoid = Sigmoid()\n",
    "Layer_Loss = MSECriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = Layer1_linear.forward(X)\n",
    "A1 = Layer1_sigmoid.forward(Z1)\n",
    "Z2 = Layer2_linear.forward(A1)\n",
    "A2 = Layer2_sigmoid.forward(Z2)\n",
    "Loss = Layer_Loss.forward(A2, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Քայլ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Առաջադրանք N10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Հաշվել առաջին ծաղիկի համար OUTPUT շերտի նեյրոնների, այսինքն՝ ցանցի output-ների գրադիենտները:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='Green'> Հուշում: Գրեք Cost function-ի բանաձևը՝ կախված ցանցի output-ներից ու Y-ներից ու ածանցեք՝ հաշվի առնելով, որ Y-ները տրված են ու փոփոխականը միայն output-ներն են:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_loss(layer2_sig, y):\n",
    "    return (layer2_sig - y)/layer2_sig.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06003895,  0.09073618],\n",
       "       [-0.06321942,  0.09065347],\n",
       "       [-0.0622405 ,  0.09069046],\n",
       "       [-0.06077643,  0.09074507],\n",
       "       [ 0.08322194, -0.05214487],\n",
       "       [ 0.08546337, -0.05217105],\n",
       "       [ 0.08703655, -0.05222023]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_loss(A2, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Առաջադրանք N11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Հաշվել output շերտի գծային շերտի նեյրոնների output-ների գրադիենտները:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='Green'> Հուշում: Հաշվի առեք, որ Cost function-ի կախվածությունը OUTPUT շերտի գծային շերտի նեյրոնների output-ներից բարդ ֆունկցիա է, հետևաբար Cost function-ի մասնակի ածանցյալը ըստ դրանց ստացվում է երկու ածանցյալների արտադրյալ՝ նախորդ քայլում հաշված output-ների գրադիենտների և երկրորդ արտադրիչը OUTPUT շերտի ոչ գծային շերտի output-ների ածանցյալն է դրա գծային շերտի output-ներից, այսինքն՝ փաստացի երկրորդ արտադրիչը սիգմոիդի ածանցյալն է ստացվում:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_output_lin(layer2_sig):\n",
    "    return (layer2_sig * (1 - layer2_sig)) * grad_loss(A2, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0146281 ,  0.02102662],\n",
       "       [-0.0155961 ,  0.02102161],\n",
       "       [-0.01530266,  0.02102386],\n",
       "       [-0.01485619,  0.02102715],\n",
       "       [ 0.02023832, -0.01208607],\n",
       "       [ 0.02054097, -0.01209472],\n",
       "       [ 0.0207202 , -0.01211097]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_output_lin(A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Առաջադրանք N12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Հաշվել W2-ի և b2-ի գրադիենտները:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='Green'> Հուշում: Այս դեպքում արդեն Cost function-ի կախվածությունը W2-ից և b2-ից \"ավելի\" բարդ ֆունկցիա է ու ածանցյալը 3 բաղադրիչների արտադրյալ է: Այդ 3 բաղադրիչներից 2-ը նախորդ առաջադրանքի արտադրիչներն են, որոնց արտադրյալը հաշվել ենք՝ \n",
    "output շերտի գծային շերտի նեյրոնների output-ների գրադիենտներն են:\n",
    "<br> Այսինքն` նորից երկու արտադրիչ ենք բազմապատկում, որոնցից մեկը արդեն հաշվել էինք, իսկ մյուսը ստանում ենք՝ OUTPUT շերտի գծային շերտի նեյրոնների output-ների բանաձևը գրելով և համապատասխանաբար ըստ W2-ի և b2-ի տարրերի ածանցելով: Կախվածությունը գծային է, հետևաբար շատ պարզ բանաձև է ստացվում:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_W2(layer1_sig):\n",
    "    return np.dot(grad_output_lin(A2).T, layer1_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00817768, -0.0052084 ,  0.00199982,  0.01078659],\n",
       "       [ 0.02453323,  0.03245409,  0.02947062,  0.01633288]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_W2(A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_b2():\n",
    "    return np.sum(grad_output_lin(A2), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00111643, 0.04780749])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_b2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Քայլ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Առաջադրանք N13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Հաշվել HIDDEN շերտի output-ների գրադիենտները:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Մաս 1__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='Green'> Հուշում: Այս դեպքում նույն սկզբունքն ենք կիրառում, ինչ-որ նախորդ առաջադրանքներում, այսինքն՝ օգտագործում ենք ածանցյալի շղթայական կանոնը: Մեզ պետք են գալու OUTPUT շերտի գծային շերտի output-ները: Սակայն մեկ բան պետք է հաշվի առնել՝ HIDDEN շերտի output-ները մի քանի ճանապարհով են ազդում Cost function-ի վրա:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Հարց. մեր դեպքում քանի՞ ճանապարհով են ազդում:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ազդում են 2 ճանապարհով\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Մաս 2__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='Green'> Հուշում: Քանի որ մեկից ավել ճանապարհով են ազդում, հետևաբար HIDDEN շերտի output-ների գրադիենտները ստացվում են ճանապարհների քանակով գումարելիների գումար:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_hidden():\n",
    "    return np.dot(grad_output_lin(A2), W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00263326,  0.01186667, -0.00279765, -0.00083201],\n",
       "       [ 0.00303094,  0.012233  , -0.00301071, -0.00092896],\n",
       "       [ 0.00291027,  0.01212217, -0.00294611, -0.00089955],\n",
       "       [ 0.00272669,  0.0119535 , -0.00284782, -0.0008548 ],\n",
       "       [-0.00636394, -0.01131638,  0.00421071,  0.00166125],\n",
       "       [-0.00648664, -0.01143398,  0.00427712,  0.00169125],\n",
       "       [-0.00655753, -0.01150696,  0.00431622,  0.00170869]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_hidden()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Առաջադրանք N14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Հաշվել HIDDEN շերտի գծային շերտի նեյրոնների output-ների գրադիենտները:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_hidden_lin(layer1_sig):\n",
    "    return layer1_sig * (1 - layer1_sig) * grad_hidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00060109,  0.00290221, -0.00064229, -0.00020736],\n",
       "       [ 0.00075426,  0.00281803, -0.00071509, -0.00022412],\n",
       "       [ 0.00071374,  0.00285351, -0.00069157, -0.00022185],\n",
       "       [ 0.00064326,  0.00288494, -0.00064794, -0.00021365],\n",
       "       [-0.00141775, -0.00278783,  0.00098302,  0.0004107 ],\n",
       "       [-0.00124771, -0.0028579 ,  0.00097207,  0.00038832],\n",
       "       [-0.00109541, -0.0028657 ,  0.00094199,  0.00035798]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_hidden_lin(A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Առաջադրանք N15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Հաշվել W1-ի և b1-ի գրադիենտները:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_W1(input_x):\n",
    "    return np.dot(grad_hidden_lin(A1).T, input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00100378,  0.00376956],\n",
       "       [ 0.0029548 ,  0.01155944],\n",
       "       [-0.0008497 , -0.0033457 ],\n",
       "       [-0.00031291, -0.00119144]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_W1(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_b1():\n",
    "    return np.sum(grad_hidden_lin(A1), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00104853,  0.00294725,  0.00020019,  0.00029002])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_b1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Քայլ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Առաջադրանք N16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Կատարել պարամետրերի update. alpha = 0.5:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_grad(W2 = W2, b2 = b2, W1 = W1, b1 = b1, alpha = 0.5):\n",
    "    delta_W2 = grad_W2(A1)\n",
    "    delta_b2 = grad_b2()\n",
    "    delta_W1 = grad_W1(X)\n",
    "    delta_b1 = grad_b1()\n",
    "    \n",
    "    W2 = W2 - alpha * delta_W2\n",
    "    b2 -= alpha * delta_b2\n",
    "    W1 -= alpha * delta_W1\n",
    "    b1 -= alpha * delta_b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.51  0.41]\n",
      " [ 0.34  0.24]\n",
      " [ 0.25 -0.14]\n",
      " [-0.62 -0.42]]\n",
      "[[ 0.50949811  0.40811522]\n",
      " [ 0.3385226   0.23422028]\n",
      " [ 0.25042485 -0.13832715]\n",
      " [-0.61984354 -0.41940428]]\n"
     ]
    }
   ],
   "source": [
    "print(W1)\n",
    "\n",
    "update_grad()\n",
    "\n",
    "print(W1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Առաջադրանք N17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Հաշվել բոլոր պարամետրերի գրադիենտները բոլոր ծաղիկների համար և կատարել update (alpha = 0.1):__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Բոլոր պարամետրերի գրադիենտները բոլոր ծաղիկների համար__ իրականում նշանակում է առանձին-առանձին բոլոր յոթ ծաղիկների համար հաշվարկված պարամետրերի գրադիենտների գումար:\n",
    "<br>Իմաստը այն է, որ հերթով բոլոր ծաղիկների համար առանձին գրադիենտները հաշվելու ու պարամետրերը update անելու փոխարեն, մենք բոլորինը միանգամից ենք update անում, այսինքն՝ յոթ անգամ _մանր_ update անելու փոխարեն, մեկ անգամ _խոշոր_ update ենք անում, որը պարունակում է բոլոր 7 update-ների ինֆորմացիան:\n",
    "<br> Խոսքը վերաբերում է միայն պարամետրերին, իսկ նեյրոնների գրադիենտները ստացվում են նեյրոնների մատրիցների չափով, այսինքն՝ HIDDEN շերտում` 7x4,  OUTPUT շերտում՝ 7x2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='Green'> Հուշում:\n",
    "Դա կարելի է անել 2 տարբերակով՝\n",
    "-  __Տարբերակ 1__\n",
    "<br> հաշվում եք յոթ ծաղիկների համար պարամետրերի գրադիենտները առանձին-առանձին և դրանք գումարում եք իրար\n",
    "-  __Տարբերակ 2__\n",
    "<br> այդ ամենը անում եք մատրիցային տեսքով<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='magenta'> ՁԵԶԱՆԻՑ ՊԱՀԱՆՋՎՈՒՄ Է ԻՐԱԿԱՆԱՑՆԵԼ ՏԱՐԲԵՐԱԿ 2-Ը:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><font color='Blue'> Ուշադրություն: OUTPUT շերտի գրադիենտները տարբեր են ստացվում առանձին ծաղիկների համար հաշվարկված գրադիենտներից, քանի որ Cost function-ի բանաձևը տարբեր է, մասնավորապես, այն բազմապատկված է 1/7-ով:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.50949811  0.40811522]\n",
      " [ 0.3385226   0.23422028]\n",
      " [ 0.25042485 -0.13832715]\n",
      " [-0.61984354 -0.41940428]]\n",
      "[[ 0.50939773  0.40773827]\n",
      " [ 0.33822712  0.23306433]\n",
      " [ 0.25050982 -0.13799258]\n",
      " [-0.61981225 -0.41928514]]\n"
     ]
    }
   ],
   "source": [
    "print(W1)\n",
    "\n",
    "update_grad(alpha = 0.1)\n",
    "\n",
    "print(W1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
